{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Sf8kIKvdh1V8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)        \n",
        "        return x"
      ],
      "metadata": {
        "id": "c7ixpH5Kt6Oa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAtt(nn.Module):\n",
        "  def __init__(self, emb_size, heads,dropout):\n",
        "    super(SelfAtt,self).__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = emb_size//heads\n",
        "    assert(self.head_dim*heads == self.emb_size), \"head_dim*heads != emb_size\"\n",
        "\n",
        "    self.query = nn.Linear(self.emb_size,self.emb_size)\n",
        "    self.key = nn.Linear(self.emb_size,self.emb_size)\n",
        "    self.value = nn.Linear(self.emb_size,self.emb_size)\n",
        "\n",
        "    self.fc_out = nn.Linear(self.head_dim*heads, self.emb_size)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,values,keys,query,mask=None):\n",
        "    N = query.shape[0]\n",
        "    values = self.value(values)\n",
        "    keys = self.key(keys)  \n",
        "    query1 = self.query(query) \n",
        "\n",
        "    values = values.view(N, -1, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    keys = keys.view(N, -1, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    query1 = query1.view(N, -1, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    energy = torch.matmul(query1, keys.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "    \n",
        "    attention = torch.softmax(energy, dim= -1)\n",
        "    print(attention)\n",
        "    x = torch.matmul(self.dropout(attention), values)    \n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    out = x.view(N, -1, self.emb_size)\n",
        "    \n",
        "    out =  self.fc_out(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "sNsNmlGst6Q7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,emb_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.att = SelfAtt(emb_size,heads,dropout)\n",
        "    self.norm1 = nn.LayerNorm(emb_size)\n",
        "    self.norm2 = nn.LayerNorm(emb_size)\n",
        "\n",
        "    self.feed_forward = PositionwiseFeedforwardLayer(emb_size, forward_expansion*emb_size, dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query):\n",
        "    att = self.att(value, key, query)\n",
        "    x = self.norm1(query + self.dropout(att))\n",
        "    forward = self.feed_forward(x)\n",
        "    out  = self.norm2(x + self.dropout(forward))\n",
        "    return out"
      ],
      "metadata": {
        "id": "XOXN93Aat6Te"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               src_vocab_size, \n",
        "               emb_size,\n",
        "               num_layers,\n",
        "               heads,\n",
        "               device,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               max_length,\n",
        "               ):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.device = device\n",
        "    self.word_embedding  = nn.Embedding(src_vocab_size,emb_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, emb_size)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "         TransformerBlock(\n",
        "             emb_size,\n",
        "             heads,\n",
        "             dropout = dropout,\n",
        "             forward_expansion = forward_expansion,\n",
        "         )\n",
        "         for _ in range(num_layers)\n",
        "        ]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([emb_size])).to(device)\n",
        "  def forward(self,x):\n",
        "    N,seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).unsqueeze(0).repeat(N, 1).to(self.device)\n",
        "    out = self.dropout(self.word_embedding(x)*self.scale + self.position_embedding(positions))\n",
        "    for layer in self.layers:\n",
        "      out  = layer(out,out,out)\n",
        "      \n",
        "    return out"
      ],
      "metadata": {
        "id": "s1B9fufTuECe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, emb_size, heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.attention = SelfAtt(emb_size,heads,dropout)\n",
        "    self.norm1 = nn.LayerNorm(emb_size)\n",
        "    self.norm = nn.LayerNorm(emb_size)\n",
        "    self.transformer_block = TransformerBlock(emb_size, heads, dropout, forward_expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, value, key, trg_mask):\n",
        "    attention = self.attention(x,x,x,trg_mask)\n",
        "    query = self.norm1(self.dropout(attention) + x)\n",
        "    out  = self.transformer_block(value, key, query)\n",
        "    return out"
      ],
      "metadata": {
        "id": "nRVWfRKPuGIw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uZFojgCL5fQg"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([embed_size])).to(device)\n",
        "\n",
        "    def forward(self, x, enc_out, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(N, 1).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x)*self.scale) + self.position_embedding(positions))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DvUN9vRyJPtK"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        embed_size=5,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=1,\n",
        "        dropout=0,\n",
        "        device=\"cpu\",\n",
        "        max_length=10,\n",
        "        src_pad_idx = 0,\n",
        "        trg_pad_idx = 0,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.device = device\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        print(\"###########################trg_mask############################\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(trg_mask)\n",
        "        print(\"###########################Encoder############################\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        enc_src = self.encoder(src)\n",
        "        print(\"###########################Decoder############################\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        print(\"\")\n",
        "        out = self.decoder(trg, enc_src, trg_mask)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(device)\n",
        "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0]]).to(\n",
        "        device\n",
        "    )\n",
        "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0]]).to(device)\n",
        "\n",
        "    src_pad_idx = 1\n",
        "    trg_pad_idx = 3\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_size = 10 \n",
        "    model  = Transformer( src_vocab_size, trg_vocab_size,).to(device)\n",
        "    out = model(x, trg)\n",
        "    print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xuEHXkCuMc9",
        "outputId": "3145256c-5b4f-4741-bf70-7d777576cf93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "###########################trg_mask############################\n",
            "\n",
            "\n",
            "\n",
            "tensor([[[[ True, False, False, False, False, False, False, False],\n",
            "          [ True,  True, False, False, False, False, False, False],\n",
            "          [ True,  True,  True, False, False, False, False, False],\n",
            "          [ True,  True,  True,  True, False, False, False, False],\n",
            "          [ True,  True,  True,  True,  True, False, False, False],\n",
            "          [ True,  True,  True,  True,  True,  True, False, False],\n",
            "          [ True,  True,  True,  True,  True,  True,  True, False],\n",
            "          [ True,  True,  True,  True,  True,  True,  True, False]]]])\n",
            "###########################Encoder############################\n",
            "\n",
            "\n",
            "\n",
            "tensor([[[[0.1266, 0.1157, 0.0879, 0.0687, 0.1109, 0.0953, 0.1135, 0.1483,\n",
            "           0.1330],\n",
            "          [0.0815, 0.0943, 0.1822, 0.1760, 0.2046, 0.0615, 0.1153, 0.0267,\n",
            "           0.0578],\n",
            "          [0.1787, 0.0407, 0.0479, 0.1591, 0.2393, 0.0331, 0.0994, 0.0121,\n",
            "           0.1896],\n",
            "          [0.0191, 0.1495, 0.2256, 0.0628, 0.0250, 0.1773, 0.0505, 0.2763,\n",
            "           0.0138],\n",
            "          [0.1633, 0.0871, 0.0771, 0.0909, 0.2004, 0.0537, 0.1181, 0.0533,\n",
            "           0.1560],\n",
            "          [0.1175, 0.0596, 0.1016, 0.1644, 0.2793, 0.0401, 0.1342, 0.0120,\n",
            "           0.0913],\n",
            "          [0.2404, 0.0064, 0.0047, 0.0379, 0.3587, 0.0027, 0.0597, 0.0007,\n",
            "           0.2889],\n",
            "          [0.0378, 0.1172, 0.3092, 0.1699, 0.1089, 0.0958, 0.0953, 0.0417,\n",
            "           0.0242],\n",
            "          [0.2283, 0.0167, 0.0037, 0.0155, 0.0431, 0.0231, 0.0428, 0.0542,\n",
            "           0.5725]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.2735, 0.0411, 0.0344, 0.1584, 0.2425, 0.0281, 0.0508, 0.0323,\n",
            "           0.1389],\n",
            "          [0.1090, 0.1209, 0.1073, 0.1215, 0.1119, 0.1130, 0.1031, 0.1173,\n",
            "           0.0960],\n",
            "          [0.0648, 0.1628, 0.1034, 0.0710, 0.0700, 0.1679, 0.1442, 0.1710,\n",
            "           0.0448],\n",
            "          [0.2306, 0.0435, 0.0499, 0.1022, 0.1627, 0.0534, 0.0998, 0.0496,\n",
            "           0.2082],\n",
            "          [0.2716, 0.0422, 0.0345, 0.1599, 0.2541, 0.0266, 0.0493, 0.0313,\n",
            "           0.1306],\n",
            "          [0.0623, 0.1568, 0.1229, 0.0895, 0.0664, 0.1649, 0.1120, 0.1673,\n",
            "           0.0579],\n",
            "          [0.0972, 0.1706, 0.0970, 0.1651, 0.1304, 0.0976, 0.0711, 0.1229,\n",
            "           0.0481],\n",
            "          [0.0787, 0.1386, 0.1199, 0.1010, 0.0803, 0.1464, 0.1110, 0.1472,\n",
            "           0.0769],\n",
            "          [0.1572, 0.1082, 0.0637, 0.1013, 0.1450, 0.1016, 0.1410, 0.1071,\n",
            "           0.0749]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.0746, 0.1313, 0.1136, 0.1697, 0.0736, 0.1338, 0.0682, 0.1439,\n",
            "           0.0913],\n",
            "          [0.0602, 0.1270, 0.1392, 0.1829, 0.0676, 0.1217, 0.0532, 0.1302,\n",
            "           0.1181],\n",
            "          [0.0833, 0.1238, 0.1416, 0.1228, 0.1047, 0.1058, 0.0859, 0.1086,\n",
            "           0.1234],\n",
            "          [0.0393, 0.1399, 0.1380, 0.1336, 0.0419, 0.1745, 0.0709, 0.1750,\n",
            "           0.0869],\n",
            "          [0.0631, 0.1338, 0.1200, 0.2004, 0.0657, 0.1291, 0.0515, 0.1432,\n",
            "           0.0933],\n",
            "          [0.0910, 0.1151, 0.1335, 0.1324, 0.1044, 0.1037, 0.0820, 0.1061,\n",
            "           0.1318],\n",
            "          [0.1411, 0.0987, 0.1065, 0.1447, 0.1587, 0.0717, 0.0708, 0.0781,\n",
            "           0.1297],\n",
            "          [0.0756, 0.1209, 0.1372, 0.1485, 0.0846, 0.1156, 0.0717, 0.1194,\n",
            "           0.1264],\n",
            "          [0.1014, 0.1190, 0.1247, 0.0797, 0.1266, 0.1041, 0.1389, 0.1021,\n",
            "           0.1036]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.1056, 0.1380, 0.0827, 0.1413, 0.1004, 0.1199, 0.0890, 0.1385,\n",
            "           0.0846],\n",
            "          [0.1702, 0.0805, 0.1015, 0.1094, 0.1871, 0.0606, 0.1081, 0.0594,\n",
            "           0.1233],\n",
            "          [0.1118, 0.1075, 0.1069, 0.1253, 0.1058, 0.1146, 0.0977, 0.1168,\n",
            "           0.1135],\n",
            "          [0.1427, 0.1599, 0.0663, 0.1199, 0.1732, 0.0715, 0.1130, 0.0892,\n",
            "           0.0642],\n",
            "          [0.1131, 0.1192, 0.0887, 0.1460, 0.1036, 0.1164, 0.0861, 0.1289,\n",
            "           0.0979],\n",
            "          [0.1344, 0.1013, 0.1113, 0.1074, 0.1450, 0.0848, 0.1155, 0.0840,\n",
            "           0.1163],\n",
            "          [0.0983, 0.0729, 0.1360, 0.1236, 0.0801, 0.1306, 0.0817, 0.1163,\n",
            "           0.1606],\n",
            "          [0.1522, 0.1008, 0.1014, 0.1037, 0.1757, 0.0687, 0.1199, 0.0697,\n",
            "           0.1078],\n",
            "          [0.0941, 0.1398, 0.0892, 0.1295, 0.0895, 0.1316, 0.0930, 0.1478,\n",
            "           0.0855]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.1146, 0.1290, 0.1104, 0.1697, 0.1130, 0.0942, 0.0430, 0.1158,\n",
            "           0.1103],\n",
            "          [0.1244, 0.0724, 0.0937, 0.1060, 0.0925, 0.1290, 0.1287, 0.1135,\n",
            "           0.1397],\n",
            "          [0.1160, 0.1020, 0.1131, 0.1422, 0.1016, 0.1116, 0.0655, 0.1182,\n",
            "           0.1298],\n",
            "          [0.1269, 0.0863, 0.0953, 0.1492, 0.0938, 0.1202, 0.0650, 0.1243,\n",
            "           0.1391],\n",
            "          [0.1384, 0.1170, 0.0989, 0.1620, 0.1383, 0.0829, 0.0546, 0.0985,\n",
            "           0.1094],\n",
            "          [0.0746, 0.1024, 0.1334, 0.1104, 0.0622, 0.1581, 0.0759, 0.1529,\n",
            "           0.1300],\n",
            "          [0.0667, 0.1804, 0.1658, 0.1160, 0.0902, 0.1071, 0.0579, 0.1261,\n",
            "           0.0899],\n",
            "          [0.0854, 0.0859, 0.1176, 0.1074, 0.0640, 0.1613, 0.0915, 0.1473,\n",
            "           0.1396],\n",
            "          [0.0956, 0.1259, 0.1242, 0.1633, 0.0892, 0.1110, 0.0397, 0.1322,\n",
            "           0.1188]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.1726, 0.0541, 0.0984, 0.1678, 0.1503, 0.0724, 0.0581, 0.0724,\n",
            "           0.1541],\n",
            "          [0.0819, 0.2034, 0.0947, 0.1039, 0.0971, 0.1125, 0.1050, 0.1295,\n",
            "           0.0721],\n",
            "          [0.1223, 0.1115, 0.0677, 0.1040, 0.0971, 0.1314, 0.1402, 0.1323,\n",
            "           0.0935],\n",
            "          [0.1576, 0.0852, 0.0805, 0.1483, 0.1396, 0.0914, 0.0835, 0.0951,\n",
            "           0.1189],\n",
            "          [0.1750, 0.0575, 0.0926, 0.1780, 0.1548, 0.0696, 0.0535, 0.0721,\n",
            "           0.1468],\n",
            "          [0.0945, 0.1383, 0.1031, 0.0994, 0.0945, 0.1243, 0.1240, 0.1279,\n",
            "           0.0939],\n",
            "          [0.1017, 0.1096, 0.1368, 0.1184, 0.1050, 0.1073, 0.0919, 0.1091,\n",
            "           0.1201],\n",
            "          [0.0906, 0.1555, 0.0986, 0.0984, 0.0945, 0.1230, 0.1235, 0.1299,\n",
            "           0.0860],\n",
            "          [0.1589, 0.0695, 0.0817, 0.1329, 0.1231, 0.1033, 0.0990, 0.0998,\n",
            "           0.1319]]]], grad_fn=<SoftmaxBackward0>)\n",
            "###########################Decoder############################\n",
            "\n",
            "\n",
            "\n",
            "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [3.8074e-01, 6.1926e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [6.0037e-02, 4.6814e-02, 8.9315e-01, 0.0000e+00, 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [2.0329e-02, 1.7267e-01, 6.3264e-03, 8.0067e-01, 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [2.7584e-04, 1.6931e-03, 2.0831e-03, 9.9569e-01, 2.5371e-04,\n",
            "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [6.7140e-01, 8.2410e-02, 1.1633e-01, 2.2703e-05, 3.3178e-02,\n",
            "           9.6664e-02, 0.0000e+00, 0.0000e+00],\n",
            "          [8.3973e-02, 1.4100e-02, 1.6391e-01, 1.1623e-02, 4.8786e-01,\n",
            "           1.8091e-01, 5.7629e-02, 0.0000e+00],\n",
            "          [6.7870e-02, 7.1323e-02, 7.8986e-02, 8.9899e-02, 3.8553e-02,\n",
            "           5.9661e-01, 5.6761e-02, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.0664, 0.1251, 0.1405, 0.0900, 0.0671, 0.1467, 0.1227, 0.1411,\n",
            "           0.1004],\n",
            "          [0.0615, 0.0970, 0.1731, 0.0731, 0.0625, 0.1433, 0.1451, 0.1252,\n",
            "           0.1192],\n",
            "          [0.0824, 0.1474, 0.1094, 0.0997, 0.0904, 0.1312, 0.1248, 0.1344,\n",
            "           0.0803],\n",
            "          [0.1845, 0.0819, 0.0877, 0.1363, 0.2093, 0.0579, 0.0630, 0.0637,\n",
            "           0.1157],\n",
            "          [0.0900, 0.1454, 0.1260, 0.1126, 0.1097, 0.1125, 0.0959, 0.1208,\n",
            "           0.0871],\n",
            "          [0.1617, 0.1329, 0.0601, 0.1672, 0.1607, 0.0787, 0.0602, 0.0967,\n",
            "           0.0818],\n",
            "          [0.0570, 0.0938, 0.1871, 0.0613, 0.0646, 0.1381, 0.1735, 0.1157,\n",
            "           0.1088],\n",
            "          [0.1078, 0.1527, 0.0972, 0.1302, 0.1245, 0.1040, 0.0848, 0.1180,\n",
            "           0.0809]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5427, 0.4573, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3185, 0.3124, 0.3691, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1843, 0.2385, 0.2528, 0.3244, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1988, 0.1805, 0.2688, 0.1440, 0.2079, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1495, 0.1174, 0.1331, 0.1560, 0.2658, 0.1782, 0.0000, 0.0000],\n",
            "          [0.1356, 0.1555, 0.2443, 0.0699, 0.0878, 0.0898, 0.2169, 0.0000],\n",
            "          [0.1502, 0.1327, 0.1709, 0.1050, 0.1639, 0.1466, 0.1307, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.1032, 0.1169, 0.1115, 0.1237, 0.0961, 0.1184, 0.0910, 0.1229,\n",
            "           0.1163],\n",
            "          [0.1093, 0.1151, 0.1079, 0.1241, 0.1027, 0.1142, 0.0927, 0.1185,\n",
            "           0.1155],\n",
            "          [0.1090, 0.1012, 0.1280, 0.1122, 0.1095, 0.1072, 0.0998, 0.1062,\n",
            "           0.1270],\n",
            "          [0.1260, 0.1100, 0.0985, 0.1225, 0.1219, 0.1034, 0.0994, 0.1066,\n",
            "           0.1118],\n",
            "          [0.1540, 0.0785, 0.1102, 0.1367, 0.1488, 0.0755, 0.0660, 0.0788,\n",
            "           0.1516],\n",
            "          [0.0730, 0.1359, 0.1155, 0.1010, 0.0692, 0.1478, 0.1151, 0.1483,\n",
            "           0.0942],\n",
            "          [0.1285, 0.0991, 0.1101, 0.1234, 0.1260, 0.0969, 0.0908, 0.0990,\n",
            "           0.1261],\n",
            "          [0.1301, 0.0906, 0.1195, 0.1278, 0.1266, 0.0914, 0.0790, 0.0936,\n",
            "           0.1414]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3921, 0.6079, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1972, 0.2277, 0.5751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1374, 0.2153, 0.2099, 0.4373, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1368, 0.1385, 0.3564, 0.0940, 0.2743, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1566, 0.1536, 0.0958, 0.2410, 0.1212, 0.2318, 0.0000, 0.0000],\n",
            "          [0.0666, 0.1052, 0.2060, 0.1977, 0.1053, 0.0379, 0.2814, 0.0000],\n",
            "          [0.1149, 0.1061, 0.2438, 0.0692, 0.2129, 0.0959, 0.1572, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.1324, 0.0943, 0.1083, 0.1398, 0.1200, 0.0940, 0.0711, 0.0991,\n",
            "           0.1411],\n",
            "          [0.1297, 0.1292, 0.0923, 0.1539, 0.1357, 0.0905, 0.0632, 0.1057,\n",
            "           0.0998],\n",
            "          [0.0953, 0.1527, 0.0932, 0.1131, 0.1011, 0.1232, 0.1115, 0.1318,\n",
            "           0.0782],\n",
            "          [0.1178, 0.2083, 0.0669, 0.1675, 0.1718, 0.0722, 0.0452, 0.1018,\n",
            "           0.0483],\n",
            "          [0.1138, 0.1240, 0.1014, 0.1241, 0.1145, 0.1090, 0.0953, 0.1154,\n",
            "           0.1024],\n",
            "          [0.1302, 0.0589, 0.1315, 0.1187, 0.1015, 0.0895, 0.0740, 0.0836,\n",
            "           0.2121],\n",
            "          [0.1001, 0.2133, 0.0711, 0.1393, 0.1309, 0.0980, 0.0721, 0.1251,\n",
            "           0.0501],\n",
            "          [0.1120, 0.1073, 0.1102, 0.1159, 0.1056, 0.1128, 0.1039, 0.1131,\n",
            "           0.1193]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5165, 0.4835, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3388, 0.3287, 0.3325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2204, 0.2176, 0.2125, 0.3495, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1217, 0.1437, 0.1616, 0.3692, 0.2038, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1664, 0.1741, 0.1584, 0.1805, 0.1310, 0.1896, 0.0000, 0.0000],\n",
            "          [0.1473, 0.1344, 0.1307, 0.1405, 0.1418, 0.1824, 0.1231, 0.0000],\n",
            "          [0.0923, 0.1103, 0.1235, 0.2633, 0.1516, 0.0912, 0.1677, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.0887, 0.1112, 0.0923, 0.0956, 0.0701, 0.1492, 0.1474, 0.1398,\n",
            "           0.1057],\n",
            "          [0.0951, 0.1074, 0.0978, 0.0973, 0.0786, 0.1393, 0.1433, 0.1307,\n",
            "           0.1106],\n",
            "          [0.1044, 0.1166, 0.0840, 0.0963, 0.0921, 0.1309, 0.1575, 0.1251,\n",
            "           0.0931],\n",
            "          [0.0836, 0.1049, 0.1464, 0.0991, 0.0823, 0.1270, 0.1100, 0.1215,\n",
            "           0.1252],\n",
            "          [0.0841, 0.1227, 0.1043, 0.0946, 0.0769, 0.1427, 0.1413, 0.1370,\n",
            "           0.0965],\n",
            "          [0.0858, 0.1081, 0.0953, 0.0962, 0.0655, 0.1530, 0.1419, 0.1426,\n",
            "           0.1116],\n",
            "          [0.1049, 0.1101, 0.1054, 0.0989, 0.1013, 0.1206, 0.1380, 0.1156,\n",
            "           0.1053],\n",
            "          [0.0841, 0.1226, 0.1001, 0.0943, 0.0749, 0.1452, 0.1441, 0.1391,\n",
            "           0.0956]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3448, 0.3065, 0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2693, 0.2458, 0.2176, 0.2673, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1973, 0.1781, 0.1743, 0.1902, 0.2601, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1684, 0.1724, 0.1796, 0.1505, 0.1663, 0.1627, 0.0000, 0.0000],\n",
            "          [0.1503, 0.1336, 0.1394, 0.1234, 0.1871, 0.1423, 0.1239, 0.0000],\n",
            "          [0.1448, 0.1317, 0.1315, 0.1391, 0.1930, 0.1346, 0.1252, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.0800, 0.1252, 0.1229, 0.1333, 0.0690, 0.1346, 0.0683, 0.1452,\n",
            "           0.1216],\n",
            "          [0.0774, 0.1291, 0.1119, 0.1212, 0.0648, 0.1462, 0.0855, 0.1528,\n",
            "           0.1111],\n",
            "          [0.0517, 0.1330, 0.1208, 0.1296, 0.0375, 0.1699, 0.0543, 0.1868,\n",
            "           0.1164],\n",
            "          [0.1307, 0.1334, 0.0948, 0.1559, 0.1487, 0.0838, 0.0584, 0.1001,\n",
            "           0.0941],\n",
            "          [0.0677, 0.1370, 0.1301, 0.1838, 0.0581, 0.1189, 0.0283, 0.1508,\n",
            "           0.1253],\n",
            "          [0.0912, 0.1188, 0.1255, 0.1327, 0.0839, 0.1215, 0.0717, 0.1300,\n",
            "           0.1246],\n",
            "          [0.0642, 0.1421, 0.1071, 0.1290, 0.0504, 0.1595, 0.0686, 0.1746,\n",
            "           0.1043],\n",
            "          [0.0670, 0.1344, 0.1328, 0.1811, 0.0571, 0.1200, 0.0288, 0.1508,\n",
            "           0.1280]]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4923, 0.5077, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3545, 0.3499, 0.2956, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2394, 0.2586, 0.2275, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2299, 0.2375, 0.1729, 0.2042, 0.1555, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1728, 0.1806, 0.1499, 0.1815, 0.1393, 0.1759, 0.0000, 0.0000],\n",
            "          [0.1562, 0.1561, 0.1354, 0.1323, 0.1254, 0.1622, 0.1323, 0.0000],\n",
            "          [0.1612, 0.1665, 0.1213, 0.1440, 0.1090, 0.1703, 0.1278, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[[0.2191, 0.0647, 0.0624, 0.1086, 0.2082, 0.0592, 0.1058, 0.0579,\n",
            "           0.1142],\n",
            "          [0.2075, 0.0680, 0.0661, 0.1079, 0.1960, 0.0643, 0.1122, 0.0624,\n",
            "           0.1155],\n",
            "          [0.1956, 0.0737, 0.0838, 0.1359, 0.2130, 0.0546, 0.0611, 0.0595,\n",
            "           0.1227],\n",
            "          [0.1781, 0.0644, 0.0416, 0.0673, 0.1385, 0.0845, 0.2724, 0.0699,\n",
            "           0.0833],\n",
            "          [0.2333, 0.0630, 0.0592, 0.1185, 0.2318, 0.0503, 0.0812, 0.0518,\n",
            "           0.1110],\n",
            "          [0.2208, 0.0637, 0.0607, 0.1061, 0.2080, 0.0593, 0.1109, 0.0574,\n",
            "           0.1131],\n",
            "          [0.1864, 0.0778, 0.0824, 0.1241, 0.1906, 0.0658, 0.0840, 0.0679,\n",
            "           0.1209],\n",
            "          [0.2346, 0.0623, 0.0591, 0.1189, 0.2341, 0.0493, 0.0793, 0.0510,\n",
            "           0.1112]]]], grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([1, 8, 10])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Transformer_teacher_force.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}