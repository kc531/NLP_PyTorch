{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLXL0TsGFd3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QFgzjWziG76k"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "from more_itertools import locate\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_pgEbr8FGGQU"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"A lambda function is a small anonymous function\",\n",
        "    \"A lambda function can take any number of arguments, but can only have one expression\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipOV6GrHGGTQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViiQs0KGGGVs"
      },
      "outputs": [],
      "source": [
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "word_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W_DqN2ASH3rt"
      },
      "outputs": [],
      "source": [
        "char_freqs = defaultdict(int)\n",
        "subwords_freqs = defaultdict(int)\n",
        "for word, freq in word_freqs.items():\n",
        "    for i in range(len(word)):\n",
        "        char_freqs[word[i]] += freq\n",
        "        # Loop through the subwords of length at least 2\n",
        "        for j in range(i + 2, len(word) + 1):\n",
        "            subwords_freqs[word[i:j]] += freq\n",
        "\n",
        "# Sort subwords by frequency\n",
        "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S4OgR-zhGGY2"
      },
      "outputs": [],
      "source": [
        "init_vocab_size = 200\n",
        "token_freqs = list(char_freqs.items()) + sorted_subwords[: init_vocab_size - len(char_freqs)]\n",
        "token_freqs = {token: freq for token, freq in token_freqs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eGUQ-iSuGGbx"
      },
      "outputs": [],
      "source": [
        "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sg7SefoFGGed"
      },
      "outputs": [],
      "source": [
        "OPEN_BRACKET = \"{\"\n",
        "CLOSED_BRACKET = \"}\"\n",
        "EMPTY_STRING = \"\"\n",
        "\n",
        "def create_tokens(s, i=0, out=EMPTY_STRING,permuts = []):\n",
        "    if i == len(s):\n",
        "      permuts.append(out)\n",
        "    for j in reversed(range(i, len(s))):\n",
        "      substr = OPEN_BRACKET + s[i:j+1] + CLOSED_BRACKET\n",
        "      create_tokens(s, j + 1, out + substr,permuts)\n",
        "    return permuts\n",
        "\n",
        "def find_in_vocab(word_vec,vocab):\n",
        "  for i in range(len(word_vec)):\n",
        "    if word_vec[i] not in vocab:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def gen_token(s, model):\n",
        "  permuts = create_tokens(s,permuts = [])\n",
        "  word_permuts  = [permuts[i].replace(\"}{\",\" \").replace(\"}\",\"\").replace(\"{\",\"\")  for i in range(len(permuts))]\n",
        "  lengths = [len(permuts[i].split(\" \"))  for i in range(len(permuts))]\n",
        "  min_len = min(lengths)\n",
        "  indexes = list(locate(lengths, lambda x: x == min_len))\n",
        "  word_permuts_new = [word_permuts[i] for i in indexes if find_in_vocab(word_permuts[i].split(\" \"),list(model.keys()))]\n",
        "  if len(word_permuts_new)==0:\n",
        "    return [\"</unkwn>\"]\n",
        "  logp = [0]*len(word_permuts_new)\n",
        "  for i in range(len(word_permuts_new)):\n",
        "    temp = 1\n",
        "    for j in word_permuts_new[i].split():\n",
        "      temp += model[j]\n",
        "    logp[i] = temp\n",
        "  return word_permuts_new[logp.index(min(logp))].split(),min(logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbBfA3KCGGhK"
      },
      "outputs": [],
      "source": [
        "print(gen_token(\"Hopefully\", model))\n",
        "print(gen_token(\"This\", model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nxU9krSVGGkJ"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model):\n",
        "    loss = 0\n",
        "    for word, freq in word_freqs.items():\n",
        "        _, word_loss = gen_token(word, model)\n",
        "        loss += freq * word_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X0ZC3w7zO9Sp"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def compute_scores(model):\n",
        "    scores = {}\n",
        "    model_loss = compute_loss(model)\n",
        "    for token, score in model.items():\n",
        "        if len(token) == 1:\n",
        "            continue\n",
        "        model_without_token = copy.deepcopy(model)\n",
        "        _ = model_without_token.pop(token)\n",
        "        scores[token] = compute_loss(model_without_token) - model_loss\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percent_to_remove = 0.1\n",
        "while len(model) > 100:\n",
        "    scores = compute_scores(model)\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
        "    # Remove percent_to_remove tokens with the lowest scores.\n",
        "    for i in range(int(len(model) * percent_to_remove)):\n",
        "        _ = token_freqs.pop(sorted_scores[i][0])\n",
        "\n",
        "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ],
      "metadata": {
        "id": "BuD7lrYET0OB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, model):\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
        "    encoded_words = [gen_token(word, model)[0] for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "tokenize(\"This is the Hugging Face course.\", model)"
      ],
      "metadata": {
        "id": "HVMiYuAcUBUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Unigram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}