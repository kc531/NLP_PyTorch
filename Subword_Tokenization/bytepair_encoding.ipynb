{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bytepair_encoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "\n",
        "path  = '/content/sample1.txt'\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fs:\n",
        "        for l in fs:\n",
        "            sent = l.strip().split(\" \")\n",
        "            for word in sent:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "vocab = get_vocab(path)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooRRmiI8RgrN",
        "outputId": "49df383f-d596-4e1a-9d81-03407e5f93e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pair_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "sM1hRnb-TmF1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = get_pair_stats(vocab)\n",
        "print(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO961YVvTkBV",
        "outputId": "ac9dc4d1-88fd-4b27-ff15-45a6172dd0ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {('T', 'h'): 1, ('h', 'e'): 10, ('e', '</w>'): 13, ('l', 'e'): 2, ('e', 'a'): 1, ('a', 'd'): 1, ('d', 'i'): 2, ('i', 'n'): 4, ('n', 'g'): 3, ('g', '</w>'): 2, ('m', 'e'): 3, ('e', 'm'): 1, ('m', 'b'): 1, ('b', 'e'): 1, ('e', 'r'): 5, ('r', 's'): 2, ('s', '</w>'): 3, ('o', 'f'): 4, ('f', '</w>'): 4, ('t', 'h'): 8, ('C', 'l'): 1, ('l', 'u'): 1, ('u', 'b'): 1, ('b', ','): 1, (',', '</w>'): 7, ('n', 'a'): 2, ('a', 'm'): 1, ('e', 'l'): 2, ('l', 'y'): 1, ('y', '</w>'): 4, ('P', 'r'): 1, ('r', 'e'): 3, ('e', 's'): 1, ('s', 'i'): 2, ('i', 'd'): 1, ('d', 'e'): 2, ('e', 'n'): 2, ('n', 't'): 2, ('t', '</w>'): 1, ('B', 'a'): 1, ('a', 'r'): 3, ('r', 'b'): 1, ('b', 'i'): 1, ('i', 'c'): 1, ('c', 'a'): 2, ('a', 'n'): 9, ('n', ','): 4, ('S', 'e'): 1, ('e', 'c'): 2, ('c', 'r'): 1, ('e', 't'): 3, ('t', 'a'): 1, ('r', 'y'): 1, ('M', 'a'): 2, ('s', 't'): 2, ('t', 'o'): 3, ('o', 'n'): 4, ('a', 'j'): 1, ('j', 'o'): 1, ('o', 'r'): 3, ('r', '</w>'): 1, ('E', 'l'): 1, ('l', 'p'): 1, ('p', 'h'): 1, ('h', 'i'): 1, ('n', 's'): 1, ('n', 'e'): 2, ('n', 'd'): 5, ('d', '</w>'): 6, ('G', 'e'): 1, ('r', 'a'): 2, ('a', 'l'): 4, ('l', '</w>'): 3, ('M', 'o'): 1, ('r', 'g'): 1, ('g', 'a'): 1, ('f', 'o'): 1, ('r', 'm'): 1, ('m', 'i'): 2, ('e', 'x'): 1, ('x', 'e'): 1, ('c', 'u'): 2, ('u', 't'): 1, ('t', 'i'): 4, ('i', 'v'): 1, ('v', 'e'): 2, ('c', 'o'): 1, ('o', 'm'): 1, ('m', 'm'): 1, ('i', 't'): 4, ('t', 't'): 1, ('t', 'e'): 2, ('e', 'e'): 2, ('e', ','): 1, ('l', 'd'): 1, ('s', 'e'): 1, ('e', 'v'): 1, ('g', 's'): 1, ('o', '</w>'): 1, ('i', 's'): 1, ('s', 'c'): 1, ('u', 's'): 1, ('s', 's'): 1, ('s', 'h'): 1, ('h', 'a'): 1, ('a', 'p'): 1, ('p', 'e'): 1, ('m', 'a'): 1, ('a', 't'): 2, ('r', 'i'): 1, ('i', 'a'): 1, ('b', 'u'): 1, ('u', 'l'): 1, ('l', 'l'): 1, ('t', ','): 1, ('t', 'u'): 1, ('u', 'r'): 1, ('p', 'o'): 2, ('o', 's'): 1, ('i', 'o'): 1, ('n', '</w>'): 1, ('n', 'n'): 1, ('n', 'o'): 1, ('q', 'u'): 2, ('u', 'a'): 2, ('t', 'y'): 2, ('l', 'i'): 1, ('o', 'w'): 1, ('w', 'd'): 1, ('r', '.'): 1, ('.', '</w>'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(pair,v_in):\n",
        "  v_out = {}\n",
        "  for word in v_in:\n",
        "    w_out = re.sub(' '.join(pair), ''.join(pair),word)\n",
        "    v_out[w_out] = v_in[word]\n",
        "  return v_out"
      ],
      "metadata": {
        "id": "93prLgSwfbLd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tokens(vocab):\n",
        "    tokens = collections.defaultdict(int)\n",
        "    vocab_tokens = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens[token] += freq\n",
        "        vocab_tokens[''.join(word_tokens)] = word_tokens\n",
        "    return tokens, vocab_tokens"
      ],
      "metadata": {
        "id": "iNb-L82tzjom"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "XDZTElqn5Ugl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_merges = 10\n",
        "print(\"Before Merging\")\n",
        "tokens, vocab_tokens = extract_tokens(vocab)\n",
        "print('All tokens: {}'.format(tokens.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens.keys())))\n",
        "print(5*'==========')\n",
        "for i in range(n_merges):\n",
        "  pairs = get_pair_stats(vocab)\n",
        "\n",
        "  if not pairs:\n",
        "    break\n",
        "\n",
        "  best_pair = max(pairs, key = pairs.get)\n",
        "  print('Best Pair : {}, count : {}'.format(best_pair,pairs[best_pair]))\n",
        "  vocab = merge(best_pair,vocab)\n",
        "  tokens, vocab_tokens = extract_tokens(vocab)\n",
        "  print('All tokens: {}'.format(tokens.keys()))\n",
        "  print('Number of tokens: {}'.format(len(tokens.keys())))\n",
        "  print('==========')\n"
      ],
      "metadata": {
        "id": "5EQO4aY2inYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)"
      ],
      "metadata": {
        "id": "KUnHyoRN2IFR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tokens_tuple = sorted(tokens.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]"
      ],
      "metadata": {
        "id": "Cwq3lfo2xiLQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    if string == '':\n",
        "        return ' '\n",
        "    if sorted_tokens == []:\n",
        "        return unknown_token\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token, string)]\n",
        "        matched_position = matched_positions[0]\n",
        "        left_substring = tokenize_word(string[0:matched_position[0]], sorted_tokens, unknown_token='</u>')\n",
        "        right_substring = tokenize_word(string[matched_position[1]:len(string)], sorted_tokens, unknown_token='</u>')\n",
        "        break\n",
        "    return left_substring + ' ' + string[matched_position[0]:matched_position[1]] + right_substring"
      ],
      "metadata": {
        "id": "WEOzA-oPGlac"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        #token_reg = re.escape(token.replace('.', '[.]'))\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token, string)]\n",
        "        #print(matched_positions)\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "        print(substring_end_positions)\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\"\"\""
      ],
      "metadata": {
        "id": "UxcKTMD64K9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_word(\"the\", sorted_tokens, unknown_token='</u>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0voXRpp84LAp",
        "outputId": "d761ee51-1ffc-424f-f219-e95821d15cc1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  the</w> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}