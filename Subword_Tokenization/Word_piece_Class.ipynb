{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_piece_Class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "class WordPiece():\n",
        "  \n",
        "  def __init__(self,n_merges,data_path, min_ouccurance_count):\n",
        "    # Number of times to be iterated\n",
        "    self.n_merges = n_merges\n",
        "    # path to the data file\n",
        "    self.path =  data_path\n",
        "    self.min_ouccurance_count = min_ouccurance_count\n",
        "    # Builds tokens from the data file\n",
        "    self.sorted_tokens = self.generate_tokens()\n",
        "\n",
        "  # Splits sentences to words based on spaces and adds </w> token at the end of each word and space between each character of the word\n",
        "  def get_vocab(self,filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fs:\n",
        "        for l in fs:\n",
        "            sent = l.strip().split(\" \")\n",
        "            for word in sent:\n",
        "                #vocab is dictionary which has words along with its frequency of ocuurance\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "  # Finds the frequency of occurance of 2 consecutive tokens in the vocab\n",
        "  def get_pair_stats(self,vocab,tokens):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    prob = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    pair_values = np.array(list(pairs.values()))\n",
        "    vocab_sum = sum(list(tokens.values()))\n",
        "    pair_prob = pair_values/vocab_sum  #np.sum(pair_values)\n",
        "    pair_keys = list(pairs.keys())\n",
        "    keys_prob = []\n",
        "    pair_likelihood = collections.defaultdict(float)\n",
        "    for i in range(len(pair_keys)):\n",
        "      k1,k2 = pair_keys[i][0], pair_keys[i][1] \n",
        "      p1,p2 = tokens[k1]/vocab_sum, tokens[k2]/vocab_sum\n",
        "      likelihood = pair_prob[i] - p1 - p2\n",
        "      pair_likelihood[pair_keys[i]] = likelihood\n",
        "    return pair_likelihood\n",
        "\n",
        "\n",
        "  # merges 2 consecutive tokens of a word in the vocab\n",
        "  def merge(self,pair,v_in):\n",
        "    v_out = {}\n",
        "    for word in v_in:\n",
        "      w_out = re.sub(' '.join(pair), ''.join(pair),word)\n",
        "      v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "  #extracts tokens and its frequency from the vocab\n",
        "  #extracts words and its corresponding token from the vocab\n",
        "  def extract_tokens(self,vocab):\n",
        "    tokens = collections.defaultdict(int)\n",
        "    vocab_tokens = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens[token] += freq\n",
        "        vocab_tokens[''.join(word_tokens)] = word_tokens\n",
        "    return tokens, vocab_tokens\n",
        "\n",
        "  #calculates length of the token\n",
        "  def measure_token_length(self,token):\n",
        "      if token[-4:] == '</w>':\n",
        "          return len(token[:-4]) + 1\n",
        "      else:\n",
        "          return len(token)\n",
        "  #generates subword tokens for the word from the data in a iterative approach\n",
        "  def generate_tokens(self):\n",
        "    vocab = self.get_vocab(self.path)\n",
        "    n_merges  = self.n_merges\n",
        "    #print(\"Before Merging\")\n",
        "    tokens, vocab_tokens = self.extract_tokens(vocab)\n",
        "    #print('All tokens: {}'.format(tokens.keys()))\n",
        "    #print('Number of tokens: {}'.format(len(tokens.keys())))\n",
        "    #print(5*'==========')\n",
        "    n_tokens = [len(tokens.keys())]\n",
        "    for i in range(n_merges):\n",
        "      pairs = self.get_pair_stats(vocab,tokens)\n",
        "      if not pairs:\n",
        "        break\n",
        "      #extracts max freq co-occured tokens from the vocab\n",
        "      best_pair = max(pairs, key = pairs.get)\n",
        "      #print('Best Pair : {}, count : {}'.format(best_pair,pairs[best_pair]))\n",
        "      #merges max co-occured token in the vocab\n",
        "      vocab = self.merge(best_pair,vocab)\n",
        "      #Extracts subword tokens from the vocab \n",
        "      tokens, vocab_tokens = self.extract_tokens(vocab)\n",
        "      #print('All tokens: {}'.format(tokens.keys()))\n",
        "      #print('Number of tokens: {}'.format(len(tokens.keys())))\n",
        "      #print('==========')\n",
        "      n_tokens.append(len(tokens.keys()))\n",
        "    #sorts subword tokens based on its length in a decresing order\n",
        "    sorted_tokens_tuple = sorted(tokens.items(), key=lambda item: (self.measure_token_length(item[0]), item[1]), reverse=True)\n",
        "    sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "    #plt.plot(n_tokens)\n",
        "    return sorted_tokens\n",
        "  #generates token for new word from the existing set of tokens created from the data\n",
        "  #A recursive function that identifies subword tokens for a word  \n",
        "  def tokenize_word(self,string, unknown_token='</u>'):\n",
        "      sorted_tokens = self.sorted_tokens\n",
        "      left_substring = \"\"\n",
        "      right_substring = \"\"\n",
        "      matched_position = []\n",
        "      if string == \"\":\n",
        "          return ''\n",
        "      if sorted_tokens == []:\n",
        "          return unknown_token\n",
        "\n",
        "      string_tokens = []\n",
        "      for i in range(len(sorted_tokens)):\n",
        "          token = sorted_tokens[i]\n",
        "          matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token, string)]\n",
        "          if len(matched_positions)!=0 :\n",
        "            matched_position = matched_positions[0]\n",
        "            left_substring = self.tokenize_word(string[0:matched_position[0]], unknown_token)\n",
        "            right_substring = self.tokenize_word(string[matched_position[1]:len(string)],unknown_token)\n",
        "            break\n",
        "      return left_substring + \" \" + string[matched_position[0]:matched_position[1]] + \" \" + right_substring\n",
        "  #performs subword tokenisation for a sentence\n",
        "  def tokenize_sentence(self,sentence,unknown_token='</u>'):\n",
        "    token  = \"\"\n",
        "    for word in sentence.split():\n",
        "      word += \"</w>\"\n",
        "      if word == \"\" or word == \" \":\n",
        "        continue\n",
        "      token+=self.tokenize_word(word)\n",
        "    return np.array(token.split(\" \")).T"
      ],
      "metadata": {
        "id": "AH6CxxpTOFV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path  = '/content/sample.txt'\n",
        "min_ouccurance_count = 2\n",
        "byte_pair_encoding = WordPiece(500,path, min_ouccurance_count)"
      ],
      "metadata": {
        "id": "ooRRmiI8RgrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair_encoding.sorted_tokens "
      ],
      "metadata": {
        "id": "E4SXYnbjwSnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair_encoding.tokenize_word(\"Cambridge</w>\")"
      ],
      "metadata": {
        "id": "iC0LtzjZbyOv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ad13f51-22db-4818-c08c-6373df888b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Cambridg  e  </w> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair_encoding.tokenize_sentence(\"this is a notes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBmdkTPGgGmn",
        "outputId": "c69090fe-9046-4c6f-996f-9a8e6795bd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', 't', '', 'his</w>', '', 'is</w>', '', 'a', '', '</w>', '', 'n',\n",
              "       '', 'o', '', 't', '', 'e', '', 's</w>', ''], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}